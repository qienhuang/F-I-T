# Grokking 的 FIT 相变映射

状态：**核心邻接应用**
目的：将 ML 中的"grokking"现象映射到 FIT 相位结构
受众：ML 研究者、AI 安全从业者、FIT 读者

导航：[`核心索引`](./README.md) | [`两页卡`](./fit_two_page_card.md) | [`Φ₃ 稳定性`](./phi3_stability.md) | [`Φ₃ 后分岔`](./FIT_Core_Extension_Post_Phi3.md)

记号：相位写作 `Φ₁/Φ₂/Φ₃`（在文件名/代码中用 `Phi1/Phi2/Phi3`）。

---

## 什么是 Grokking？

Grokking 是神经网络训练中观察到的现象：

1. 模型快速记住训练数据（训练损失接近零）
2. 测试准确率在很长一段时间内保持在随机水平
3. 经过更多训练步骤后，测试准确率突然提升到接近完美

这种记忆与泛化之间的延迟就是"grokking"现象。

---

## FIT 变量映射

| FIT 变量 | Grokking 解释 |
|----------|--------------|
| **力（F）** | 梯度下降更新；优化压力 |
| **信息（I）** | 学到的表示；持久存在并影响未来预测的权重配置 |
| **时间（T）** | 训练步数（epochs）；参数更新的节奏 |
| **约束（C）** | 累积的结构，限制有效假设空间 |

---

## 相位映射

### Φ₁ — 积累（训练早期）

**特征：**
- 随机初始化；无稳定结构
- 力（梯度）大且嘈杂
- 信息不持久（权重快速变化）
- 尚无有意义的约束

**可观测标志：**
- 训练和测试损失都很高
- 表示是随机/无结构的

**持续时间：** 很短（通常 < 100 步）

---

### Φ₂ — 结晶（记忆化）

**特征：**
- 局部结构稳定（记忆训练样本）
- 力被局部吸收但非全局吸收
- 信息是样本特定的，不可泛化
- 约束是局部的（每个样本有自己的"解"）

**可观测标志：**
- 训练损失 → 0
- 测试准确率保持在随机水平
- 表示是高维的、不可迁移的

**持续时间：** 可能很长（10³–10⁵ 步，取决于任务/超参数）

**为什么 Φ₂ 会持续：**
记忆化是一个**稳定态**。模型找到了一个满足训练目标的局部吸引子。没有梯度压力去改变——损失已经是零了。

转向 Φ₃ 需要不同的机制：**权重衰减**（或类似正则化），它缓慢侵蚀高复杂度的记忆解，产生向更简单、可泛化结构的压力。

---

### Φ₂ → Φ₃ 转变（Grokking 事件）

**PT-MSS（相变最小信号集）检验：**

| 信号类别 | Grokking 表现 |
|---------|--------------|
| 力的重新分布 | 梯度流从拟合单个样本转向强化共享结构 |
| 信息重新编码 | 表示从样本特定转向特征基础（如模运算的傅里叶模式） |
| 约束重组 | 有效假设类从高维记忆塌缩到低维泛化 |

三个信号在窄训练窗口内共同出现 → **相变确认**。

**为什么看起来很突然：**
转变在内部动力学中并不突然——结构对齐是逐渐累积的。但可观测指标（测试准确率）只在对齐越过阈值时才变化。指标滞后于结构。

---

### Φ₃ — 协调（泛化）

**特征：**
- 力被全局吸收；模型有了连贯的解
- 信息可跨样本复用（可泛化表示）
- 约束强且稳定（低有效维度）

**可观测标志：**
- 训练损失 ≈ 0
- 测试准确率 ≈ 100%
- 表示显示结构化模式（如模加法的傅里叶基）

**稳定性准则（来自 [`phi3_stability.md`](./phi3_stability.md)）：**

| 准则 | Grokking 解释 |
|------|--------------|
| SC-1（持久性） | 测试准确率在延长训练中保持高位 |
| SC-2（扰动韧性） | 小权重扰动后准确率恢复 |
| SC-3（迁移稳定性） | 学到的特征迁移到相关任务 |

---

## Φ₃ 之后：Grokking 之后会发生什么？

根据 [Φ₃ 后分岔](./FIT_Core_Extension_Post_Phi3.md)，存在两条路径：

### 路径 A — 对泛化的过拟合

悖论性地，grokking 后继续训练可能降低性能：

- **约束硬化**：模型变得过于僵化，失去对分布偏移的适应能力
- **力的同质化**：持续梯度下降强化现有结构而不探索替代方案
- **信息僵化**：表示被锁定在特定任务格式上

**结果：** 脆弱的泛化；在轻微任务变化下失败。

### 路径 B — 元学习 / 迁移

如果训练机制刻意引入变化：

- **(B1) 力的提升**：新任务或领域引入更高层次的优化压力
- **(B2) 信息重新分层**：模型学会学习，而不仅仅是解决问题
- **(B3) 约束包络**：任务特定的解成为更一般结构的子模块

**结果：** 稳健的迁移；对新任务的适应能力。

---

## 对 AI 安全的启示

### 1. Grokking 在结构上是可预测的

FIT 表明 grokking 并不神秘——它是一个具有可识别前兆的相变。延迟不是低效，而是**结构必然**。

### 2. Grokking 后才是真正的风险区

已经"grok"的模型处于 Φ₃。问题是：接下来会发生什么？

- 如果训练停止：模型稳定但冻结
- 如果训练继续但无多样性：路径 A（脆弱性）
- 如果训练继续且有刻意探索：路径 B（适应性）

### 3. 空性窗口作为安全机制

"受控涅槃"论文提出利用 grokking 后的状态（Φ₃）来插入被结构锁定的安全约束。这行得通是因为：

- Φ₃ 约束难以逆转（MCC-6）
- 在 Φ₃ 期间插入安全结构使其成为稳定吸引子的一部分
- 后续训练无法轻易移除它，除非重新触发相变

---

## Scaling Law 联系

Li² 实验证明 grokking 时间按以下规律缩放：

> n_grok ~ M · log(M)

其中 M 是模数（任务复杂度），n_grok 是所需训练样本数。

用 FIT 术语：
- **M** → 状态空间大小（约束域）
- **log(M)** → 可泛化结构的有效信息量
- **n_grok ~ M · log(M)** → 相变需要足够的力暴露来填充信息结构

这与 FIT 的观点一致：相变需要**阈值累积**，而非连续改进。

---

## 总结表

| 训练阶段 | FIT 相位 | 可观测 | 内部结构 |
|---------|---------|-------|---------|
| 训练早期 | Φ₁ | 高损失 | 无稳定结构 |
| 记忆化 | Φ₂ | 训练=0, 测试=随机 | 局部、高维 |
| Grokking 事件 | Φ₂→Φ₃ | 测试准确率跳升 | PT-MSS 满足 |
| 泛化 | Φ₃ | 训练=0, 测试≈100% | 全局、低维 |
| Grokking 后 | Φ₃→? | 取决于训练机制 | 路径 A 或 路径 B |

---

## 参考文献

- Power et al. (2022). "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
- Tian et al. (2025). "Provable Scaling Laws of Feature Emergence from Learning Dynamics of Grokking" ([repo](https://github.com/yuandong-tian/understanding))
- FIT v2.4 规范：[`docs/v2.4.md`](../v2.4.md)
- 受控涅槃论文：[`papers/controlled_nirvana.md`](../../papers/controlled_nirvana.md)
