"Hard indicators" for grokking (and regime shifts in learning more broadly) are easy to claim and hard to validate, largely because many evaluation setups become non-evaluable: under common event definitions (e.g., fixed accuracy thresholds), the event may not occur, making predictive metrics ill-defined. We present a preregistered Explore -> Lock -> Evaluate pipeline that enforces temporal separation between indicator development and evaluation. We introduce a jump / regime-shift event definition that yields dense event occurrence in Phase B, enabling meaningful early-warning evaluation. Applying this protocol, we evaluate a baseline composite indicator under explicit false-positive-rate (FPR) control. We find a sharp failure mode: one score orientation achieves higher ranking metrics (e.g., AUC) but exhibits an uncontrollable FPR floor (\(\approx 0.44\)), rendering it invalid as an alarm; the alternative orientation admits proper FPR control but provides limited coverage at strict FPR (\(0.05\)) and a clear coverage-risk tradeoff. These results establish that ranking metrics alone are insufficient for early-warning validity and motivate treating FPR controllability as a necessary condition for "hard indicators".
